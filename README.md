# Training-Neural-Network-from-Scratch

## **Neural Network Exploration**

This project is designed to guide you through the exciting world of Neural Networks! The notebooks within this repository will equip you with the knowledge to implement Gradient Descent with Backpropagation from scratch, explore regularization techniques, and finally, train your own Neural Network model. 

## **Project Structure**

The project consists of three Jupyter Notebooks:

## 1. **Implementing Gradient Descent with Backpropagation.ipynb:**  
![image](https://github.com/user-attachments/assets/ede26eb8-759e-4fb4-a194-01187351aa06)

   ### This notebook delves into the core concept of Gradient Descent and its collaboration with Backpropagation. You'll build a Neural Network architecture from scratch using NumPy and train it on a dataset, manually implementing the backpropagation algorithm to update the network weights.
   
 ![image](https://github.com/user-attachments/assets/d0155036-785c-47c8-b0c6-6a93cbd4f301)




## 2. **Implementing Regularization.ipynb:** 
   This notebook introduces regularization techniques, L1 and L2, used to prevent overfitting in Neural Networks. You'll define functions to calculate both L1 and L2 regularization penalties in NumPy, and explore their implementation within the PyTorch framework.

## 3. **Training a Neural Network.ipynb:** 
   (**Starter Notebook**): This notebook serves as a starting point for building and training your own Neural Network model. It provides a basic structure to get you familiar with the process. You'll be guided through defining the model architecture, loading a dataset, and implementing the training loop. 

## **Getting Started**

1. **Prerequisites:** Ensure you have Python installed on your system along with the following libraries: NumPy, matplotlib, PyTorch, torchvision, and tqdm. You can install them using pip:

   ```bash
   pip install numpy matplotlib pillow torch torchvision tqdm
   ```

2. **Clone or Download:** Clone this repository or download the project files.

3. **Navigate:** Open a terminal or command prompt and navigate to the project directory.

**Using the Notebooks**

1. **Jupyter Notebook:**  
   - Launch a Jupyter Notebook environment (e.g., using `jupyter notebook`)
   - Open the desired notebook (e.g., `Implementing Gradient Descent with Backpropagation.ipynb`).

2. **Run the Cells:**  
   - Execute the code cells (blocks of code) one by one by pressing `Shift + Enter` or clicking the "Run" button in the toolbar.

## **Further Exploration**

Feel free to experiment within the notebooks! Here are some ideas:

* Modify the hyperparameters (learning rate, number of epochs) in the Gradient Descent notebook and observe the impact on training.
* Explore different network architectures in the Training a Neural Network notebook (e.g., number of layers, neurons per layer).
* Implement different activation functions beyond sigmoid.
* Try out various datasets beyond the one used in the starter notebook.

## **Resources**

* Deep Learning with PyTorch: [https://www.manning.com/books/deep-learning-with-pytorch](https://www.manning.com/books/deep-learning-with-pytorch)
* Neural Networks and Deep Learning: [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)


I hope this README provides a clear roadmap for your exploration of Neural Networks!
